# ğŸµ Audio Sentiment Classification with CNN

This repository contains an experiment in classifying **audio sentiment** (happy, sad, etc.) using extracted audio features and a custom **1D Convolutional Neural Network (CNN)**.

---

# ğŸ“Š Dataset

The model is trained using the [Persian-Speech-Dataset](https://huggingface.co/datasets/SeyedAli/Persian-Speech-Dataset) available on Hugging Face. This dataset contains Persian audio clips with corresponding transcriptions and emotion labels.  

**Dataset details:**

- `audio` ğŸ§: Path to the audio file  
- `transcription` ğŸ“: Text transcription of the audio  
- `emotion` ğŸ˜„ğŸ˜¢ğŸ˜¡ğŸ˜: Emotion label (e.g., happy, sad, angry, neutral)  

*Note: The dataset is licensed under the MIT License.*

---

# ğŸ› ï¸ Project Overview

- **Feature Extraction** ğŸ”:  
  Extracted features from audio files, including:  
  - **MFCC (Mel-Frequency Cepstral Coefficients)**  
  - **Chroma**  
  - **Zero Crossing Rate**  
  - **Spectral Rolloff**  
  - **Tonnetz (harmonic features)**  
  - Additional audio features  

- **Data Augmentation** âœ¨:  
  Applied several augmentations to improve model generalization:  
  - Noise injection ğŸ”Š  
  - Time shifting â©  
  - Time stretching â±ï¸  
  - Pitch shifting ğŸµ  

- **Data Preparation** ğŸ“‚:  
  - Features are stored in a Pandas DataFrame  
  - Split into training and testing sets  

- **Model** ğŸ§ :  
  - Custom **1D CNN architecture** trained on the feature vectors  
  - **Validation Accuracy** âœ…: ~85%  
  - **Test Accuracy** ğŸ§ª: ~61%  

> âš ï¸ Note: The gap between validation and test accuracy suggests potential overfitting and room for improvement.

---

# âš™ï¸ How It Works

1. Load the Persian-Speech-Dataset ğŸ“¥  
2. Extract features (MFCC, Chroma, Zero Crossing Rate, Spectral Rolloff, Tonnetz, etc.) ğŸ¶  
3. Apply data augmentation for more training samples âœ¨  
4. Store features in a structured Pandas DataFrame ğŸ—‚ï¸  
5. Split data into train/test sets ğŸ”€  
6. Train a custom 1D CNN on the extracted features ğŸ§   
7. Evaluate model performance on validation and test sets ğŸ“Š  

---

# ğŸ“š Libraries Used

- **librosa** ğŸ§ â€“ audio processing and feature extraction  
- **matplotlib** ğŸ“ˆ â€“ plotting and visualization  
- **tensorflow** ğŸ¤– â€“ building and training CNN models  
- **keras** ğŸ› ï¸ â€“ high-level API for CNN architecture  
- **numpy** ğŸ”¢ â€“ numerical operations  
- **pandas** ğŸ—ƒï¸ â€“ data manipulation and storage  
- **io** ğŸ’¾ â€“ input/output operations (built-in Python library)  

---

# ğŸ““ Notebook

All steps are included in the Jupyter Notebook (`.ipynb`) provided in this repository.  
Open the notebook to explore:  
- Feature extraction ğŸ”  
- Data augmentation âœ¨  
- CNN training ğŸ§   
- Model evaluation ğŸ“Š  

---

# ğŸ’» Installation

Install dependencies using pip:

```bash
pip install librosa matplotlib tensorflow keras numpy pandas
```

